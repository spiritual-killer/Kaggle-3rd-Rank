{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":81881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:21:10.269678Z","iopub.execute_input":"2025-02-16T20:21:10.270028Z","iopub.status.idle":"2025-02-16T20:21:10.630806Z","shell.execute_reply.started":"2025-02-16T20:21:10.270001Z","shell.execute_reply":"2025-02-16T20:21:10.629894Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/1/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install bitsandbytes\n!pip install unsloth\n!pip install accelerate\n!pip install peft\n!pip install torch==2.1.2 --force-reinstall\n!pip install --upgrade transformers\n!pip install --upgrade unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:21:10.632152Z","iopub.execute_input":"2025-02-16T20:21:10.632680Z","iopub.status.idle":"2025-02-16T20:28:26.235391Z","shell.execute_reply.started":"2025-02-16T20:21:10.632643Z","shell.execute_reply":"2025-02-16T20:28:26.234360Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch._dynamo\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:28:26.237146Z","iopub.execute_input":"2025-02-16T20:28:26.237462Z","iopub.status.idle":"2025-02-16T20:28:29.257121Z","shell.execute_reply.started":"2025-02-16T20:28:26.237437Z","shell.execute_reply":"2025-02-16T20:28:29.256395Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datetime import datetime\nfrom unsloth import FastLanguageModel\nimport torch\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:28:29.258507Z","iopub.execute_input":"2025-02-16T20:28:29.258997Z","iopub.status.idle":"2025-02-16T20:28:57.074920Z","shell.execute_reply.started":"2025-02-16T20:28:29.258965Z","shell.execute_reply":"2025-02-16T20:28:57.074003Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\ntest_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nmap_dict = {\n    \"as\": \"Assamese\",\n    \"bd\": \"Bodo\",\n    \"bn\": \"Bengali\",\n    \"gu\": \"Gujarati\",\n    \"hi\": \"Hindi\",\n    \"kn\": \"Kannada\",\n    \"ml\": \"Malayalam\",\n    \"mr\": \"Marathi\",\n    \"or\": \"Odia\",\n    \"pa\": \"Punjabi\",\n    \"ta\": \"Tamil\",\n    \"te\": \"Telugu\",\n    \"ur\": \"Urdu\"\n}\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_path,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:28:57.075905Z","iopub.execute_input":"2025-02-16T20:28:57.076239Z","iopub.status.idle":"2025-02-16T20:30:22.643759Z","shell.execute_reply.started":"2025-02-16T20:28:57.076207Z","shell.execute_reply":"2025-02-16T20:30:22.642804Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0abb6b3cc2e4f0898068b2cb0e30b21"}},"metadata":{}},{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/2 does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Adding LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:30:22.644725Z","iopub.execute_input":"2025-02-16T20:30:22.645055Z","iopub.status.idle":"2025-02-16T20:30:29.501944Z","shell.execute_reply.started":"2025-02-16T20:30:22.645027Z","shell.execute_reply":"2025-02-16T20:30:29.501267Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the {} language.\n\n### Input:\n{`}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    texts = []\n    new_labels = []\n    sentences = examples['sentence']\n    langs = examples['language']\n    labels = examples['label']\n    for sent, lang, lab in zip(sentences, langs, labels):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        #text = generate_prompt(example) + EOS_TOKEN\n        new_lab = 1 if lab == 'Positive' else 0\n        text = alpaca_prompt.format(map_dict[lang], sent, new_lab) + EOS_TOKEN\n        texts.append(text)\n        new_labels.append(new_lab)\n    return { \"text\" : texts, \"label\": new_labels}\n\ndataset = load_dataset(\"csv\", data_files = [file_path])[\"train\"]\ndataset = dataset.map(formatting_prompts_func, batched = True,)\nsplit_dataset = dataset.train_test_split(test_size = 0.2, seed = 62)\ntrain_dataset, valid_dataset = split_dataset['train'], split_dataset['test']\ntrain_dataset, valid_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:30:29.502733Z","iopub.execute_input":"2025-02-16T20:30:29.502994Z","iopub.status.idle":"2025-02-16T20:30:29.921388Z","shell.execute_reply.started":"2025-02-16T20:30:29.502970Z","shell.execute_reply":"2025-02-16T20:30:29.920709Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1797dcd068484adca1493daf37f4a3b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0221117c7b1f43e882f2333d9f7f2efb"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['ID', 'sentence', 'label', 'language', 'text'],\n     num_rows: 800\n }),\n Dataset({\n     features: ['ID', 'sentence', 'label', 'language', 'text'],\n     num_rows: 200\n }))"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_dataset[0], valid_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:30:29.923481Z","iopub.execute_input":"2025-02-16T20:30:29.923694Z","iopub.status.idle":"2025-02-16T20:30:29.930431Z","shell.execute_reply.started":"2025-02-16T20:30:29.923677Z","shell.execute_reply":"2025-02-16T20:30:29.929735Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"({'ID': 150,\n  'sentence': 'ସାମ୍ପ୍ରତିକ ସପ୍ତାହଗୁଡିକରେ, ସୋନିଲିଭ୍ କିଛି ବାସ୍ତବରେ ଲୋକପ୍ରିୟ ସିରିଜ୍ ଏବଂ ଲଗାତାର ଦେଖିବା ପାଇଁ କିଛି ଭଲ ବିଷୟବସ୍ତୁ ତାଲିକାଭୁକ୍ତ କରୁଛି |',\n  'label': 1,\n  'language': 'or',\n  'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the Odia language.\\n\\n### Input:\\nସାମ୍ପ୍ରତିକ ସପ୍ତାହଗୁଡିକରେ, ସୋନିଲିଭ୍ କିଛି ବାସ୍ତବରେ ଲୋକପ୍ରିୟ ସିରିଜ୍ ଏବଂ ଲଗାତାର ଦେଖିବା ପାଇଁ କିଛି ଭଲ ବିଷୟବସ୍ତୁ ତାଲିକାଭୁକ୍ତ କରୁଛି |\\n\\n### Response:\\n1<|eot_id|>'},\n {'ID': 466,\n  'sentence': 'क्रोमा एसी का एआई मोड, जिसे ऑटो ऑपरेशन मोड के रूप में भी जाना जाता है, कमरे के तापमान के आधार पर पंखे की गति और तापमान को ऑटोमेटिकली सेट करता है। चूंकि यह आर्टिफीशियल इंटेलिजेंस पर काम करता है, इसलिए परिणाम इतने अच्छे नहीं हैं।',\n  'label': 0,\n  'language': 'hi',\n  'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPredict the sentiment of input as 1 (positive) or 0 (negative), output 0 or 1. This sentence comes from the Hindi language.\\n\\n### Input:\\nक्रोमा एसी का एआई मोड, जिसे ऑटो ऑपरेशन मोड के रूप में भी जाना जाता है, कमरे के तापमान के आधार पर पंखे की गति और तापमान को ऑटोमेटिकली सेट करता है। चूंकि यह आर्टिफीशियल इंटेलिजेंस पर काम करता है, इसलिए परिणाम इतने अच्छे नहीं हैं।\\n\\n### Response:\\n0<|eot_id|>'})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 8,\n        warmup_steps = 5,\n        num_train_epochs = 5, # Set this for 1 full training run.\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:30:29.931267Z","iopub.execute_input":"2025-02-16T20:30:29.931473Z","iopub.status.idle":"2025-02-16T20:30:35.025171Z","shell.execute_reply.started":"2025-02-16T20:30:29.931455Z","shell.execute_reply":"2025-02-16T20:30:35.024030Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3e459cd1a74efb85291e2c70939883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acbddf617a9149e6bdc752196add4ed3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8654008fe34394b9724709cdc977a3"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T20:30:35.026781Z","iopub.execute_input":"2025-02-16T20:30:35.027114Z","iopub.status.idle":"2025-02-16T21:52:54.087591Z","shell.execute_reply.started":"2025-02-16T20:30:35.027081Z","shell.execute_reply":"2025-02-16T21:52:54.086890Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 800 | Num Epochs = 5\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n\\        /    Total batch size = 16 | Total steps = 250\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 1:21:49, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.336300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.709700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.680300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.702300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.601200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.552600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.577200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.568800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.583100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.543600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.494400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.439100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.423900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.435700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.462600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.321400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.353100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.349400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.348400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.354100</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.280300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.245000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.267100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.263500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.259600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Inference Test\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\nexample = train_dataset[0]\ninputs = tokenizer(\n[\n    alpaca_prompt.format(map_dict[example['language']], example['sentence'], '')\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nresult = tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0]\nresult, example['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:52:54.088574Z","iopub.execute_input":"2025-02-16T21:52:54.088858Z","iopub.status.idle":"2025-02-16T21:52:54.806385Z","shell.execute_reply.started":"2025-02-16T21:52:54.088835Z","shell.execute_reply":"2025-02-16T21:52:54.805615Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('1', 1)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\ndef get_results(examples):\n    results = []\n    sentences = examples['sentence']\n    langs = examples['language']\n    inputs = tokenizer(\n        [\n            alpaca_prompt.format(map_dict[langs], sentences, '')\n        ], return_tensors = \"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n    result = int(tokenizer.batch_decode(outputs)[0].split('Response')[-1].split('\\n')[1][0])\n    return {'result': result}\n\ntrain_dataset = train_dataset.map(get_results)\nvalid_dataset = valid_dataset.map(get_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T21:52:54.807207Z","iopub.execute_input":"2025-02-16T21:52:54.807520Z","iopub.status.idle":"2025-02-16T22:02:34.504700Z","shell.execute_reply.started":"2025-02-16T21:52:54.807489Z","shell.execute_reply":"2025-02-16T22:02:34.503995Z"}},"outputs":[{"name":"stderr","text":"Parameter 'function'=<function get_results at 0x7ed2ffce37f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"217a5129559f4b6aa49bc1b1a0eaba95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea8e7983be042f3954cc68b3947843c"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def calculate_F1(actual, predicted):\n    tp, tn, fp, fn = 0, 0, 0, 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            if actual[i] == 1: tp += 1\n            else: tn += 1\n        else:\n            if actual[i] == 1: fn += 1\n            else: fp += 1\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    accuracy = (tp+tn) / (tp+tn+fp+fn)\n    f1_score = (2 * precision * recall) / (precision+recall)\n    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}')\n    print(f'F1 Score: {f1_score}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:02:34.974504Z","iopub.execute_input":"2025-02-16T22:02:34.974866Z","iopub.status.idle":"2025-02-16T22:02:34.980109Z","shell.execute_reply.started":"2025-02-16T22:02:34.974838Z","shell.execute_reply":"2025-02-16T22:02:34.979264Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print('Train Metrics')\ncalculate_F1(train_dataset[:]['label'], train_dataset[:]['result'])\nprint('Validation Metrics')\ncalculate_F1(valid_dataset[:]['label'], valid_dataset[:]['result'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:20:48.591483Z","iopub.execute_input":"2025-02-16T22:20:48.591862Z","iopub.status.idle":"2025-02-16T22:20:48.611459Z","shell.execute_reply.started":"2025-02-16T22:20:48.591830Z","shell.execute_reply":"2025-02-16T22:20:48.610794Z"}},"outputs":[{"name":"stdout","text":"Train Metrics\nAccuracy: 0.98875, Precision: 0.9927360774818402, Recall: 0.9855769230769231\nF1 Score: 0.9891435464414958\nValidation Metrics\nAccuracy: 0.935, Precision: 0.9148936170212766, Recall: 0.945054945054945\nF1 Score: 0.9297297297297297\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:24:26.744438Z","iopub.execute_input":"2025-02-16T22:24:26.744801Z","iopub.status.idle":"2025-02-16T22:24:27.743453Z","shell.execute_reply.started":"2025-02-16T22:24:26.744777Z","shell.execute_reply":"2025-02-16T22:24:27.742721Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/tokenizer.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files = [test_file_path])[\"train\"]\ntest_dataset = test_dataset.map(get_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:20:56.037910Z","iopub.execute_input":"2025-02-16T22:20:56.038236Z","iopub.status.idle":"2025-02-16T22:22:04.540634Z","shell.execute_reply.started":"2025-02-16T22:20:56.038210Z","shell.execute_reply":"2025-02-16T22:22:04.539941Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a1769383234915b38b0b0a9415fd1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e155cdda84475b837b1731af0763bf"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"predictions = test_dataset[:]['result']\npredictions = ['Positive' if predictions[i] == 1 else 'Negative' for i in range(len(predictions))]\nlen(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:24:33.853129Z","iopub.execute_input":"2025-02-16T22:24:33.853408Z","iopub.status.idle":"2025-02-16T22:24:33.859939Z","shell.execute_reply.started":"2025-02-16T22:24:33.853387Z","shell.execute_reply":"2025-02-16T22:24:33.859246Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"100"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"dl = pd.DataFrame({'ID': list(range(1,len(predictions)+1)), 'label': predictions})\ndl.to_csv('output.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:24:37.240223Z","iopub.execute_input":"2025-02-16T22:24:37.240512Z","iopub.status.idle":"2025-02-16T22:24:37.260521Z","shell.execute_reply.started":"2025-02-16T22:24:37.240490Z","shell.execute_reply":"2025-02-16T22:24:37.259655Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"dl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:27:06.551264Z","iopub.execute_input":"2025-02-16T22:27:06.551592Z","iopub.status.idle":"2025-02-16T22:27:06.588180Z","shell.execute_reply.started":"2025-02-16T22:27:06.551570Z","shell.execute_reply":"2025-02-16T22:27:06.587496Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"     ID     label\n0     1  Positive\n1     2  Positive\n2     3  Positive\n3     4  Positive\n4     5  Negative\n..  ...       ...\n95   96  Positive\n96   97  Positive\n97   98  Negative\n98   99  Positive\n99  100  Negative\n\n[100 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>96</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>97</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>98</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>99</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>100</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"full_train=dataset.map(get_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:32:55.394688Z","iopub.execute_input":"2025-02-16T22:32:55.395070Z","iopub.status.idle":"2025-02-16T22:42:03.517339Z","shell.execute_reply.started":"2025-02-16T22:32:55.395041Z","shell.execute_reply":"2025-02-16T22:42:03.516612Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38f1b26b3beb45f0984da324b761cf48"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}