{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-17T05:11:13.622309Z",
     "iopub.status.busy": "2025-02-17T05:11:13.622036Z",
     "iopub.status.idle": "2025-02-17T05:11:13.633689Z",
     "shell.execute_reply": "2025-02-17T05:11:13.632820Z",
     "shell.execute_reply.started": "2025-02-17T05:11:13.622276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-17T05:11:13.634745Z",
     "iopub.status.busy": "2025-02-17T05:11:13.634481Z",
     "iopub.status.idle": "2025-02-17T05:14:15.478342Z",
     "shell.execute_reply": "2025-02-17T05:14:15.477148Z",
     "shell.execute_reply.started": "2025-02-17T05:11:13.634719Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-17T05:14:15.480649Z",
     "iopub.status.busy": "2025-02-17T05:14:15.480280Z",
     "iopub.status.idle": "2025-02-17T05:16:40.024352Z",
     "shell.execute_reply": "2025-02-17T05:16:40.023412Z",
     "shell.execute_reply.started": "2025-02-17T05:14:15.480608Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:17:38.177408Z",
     "iopub.status.busy": "2025-02-17T05:17:38.177091Z",
     "iopub.status.idle": "2025-02-17T05:18:03.989308Z",
     "shell.execute_reply": "2025-02-17T05:18:03.988379Z",
     "shell.execute_reply.started": "2025-02-17T05:17:38.177379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:03.990673Z",
     "iopub.status.busy": "2025-02-17T05:18:03.990435Z",
     "iopub.status.idle": "2025-02-17T05:18:03.994712Z",
     "shell.execute_reply": "2025-02-17T05:18:03.993869Z",
     "shell.execute_reply.started": "2025-02-17T05:18:03.990652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:03.996698Z",
     "iopub.status.busy": "2025-02-17T05:18:03.996280Z",
     "iopub.status.idle": "2025-02-17T05:18:04.011204Z",
     "shell.execute_reply": "2025-02-17T05:18:04.010443Z",
     "shell.execute_reply.started": "2025-02-17T05:18:03.996677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:04.012452Z",
     "iopub.status.busy": "2025-02-17T05:18:04.012185Z",
     "iopub.status.idle": "2025-02-17T05:18:04.027747Z",
     "shell.execute_reply": "2025-02-17T05:18:04.027167Z",
     "shell.execute_reply.started": "2025-02-17T05:18:04.012433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "print(\"Current device: \", torch.cuda.current_device())\n",
    "print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:04.028661Z",
     "iopub.status.busy": "2025-02-17T05:18:04.028413Z",
     "iopub.status.idle": "2025-02-17T05:18:04.503729Z",
     "shell.execute_reply": "2025-02-17T05:18:04.502977Z",
     "shell.execute_reply.started": "2025-02-17T05:18:04.028641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:04.504979Z",
     "iopub.status.busy": "2025-02-17T05:18:04.504648Z",
     "iopub.status.idle": "2025-02-17T05:18:04.519756Z",
     "shell.execute_reply": "2025-02-17T05:18:04.518984Z",
     "shell.execute_reply.started": "2025-02-17T05:18:04.504926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:04.520962Z",
     "iopub.status.busy": "2025-02-17T05:18:04.520649Z",
     "iopub.status.idle": "2025-02-17T05:18:04.532923Z",
     "shell.execute_reply": "2025-02-17T05:18:04.532257Z",
     "shell.execute_reply.started": "2025-02-17T05:18:04.520913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "map_dict = {\n",
    "    \"as\": \"Assamese\",\n",
    "    \"bd\": \"Bodo\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"gu\": \"Gujarati\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"kn\": \"Kannada\",\n",
    "    \"ml\": \"Malayalam\",\n",
    "    \"mr\": \"Marathi\",\n",
    "    \"or\": \"Odia\",\n",
    "    \"pa\": \"Punjabi\",\n",
    "    \"ta\": \"Tamil\",\n",
    "    \"te\": \"Telugu\",\n",
    "    \"ur\": \"Urdu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:18:16.374792Z",
     "iopub.status.busy": "2025-02-17T05:18:16.374406Z",
     "iopub.status.idle": "2025-02-17T05:19:37.483962Z",
     "shell.execute_reply": "2025-02-17T05:19:37.483021Z",
     "shell.execute_reply.started": "2025-02-17T05:18:16.374752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:19:37.485325Z",
     "iopub.status.busy": "2025-02-17T05:19:37.485078Z",
     "iopub.status.idle": "2025-02-17T05:19:37.489389Z",
     "shell.execute_reply": "2025-02-17T05:19:37.488674Z",
     "shell.execute_reply.started": "2025-02-17T05:19:37.485303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:19:51.157484Z",
     "iopub.status.busy": "2025-02-17T05:19:51.157214Z",
     "iopub.status.idle": "2025-02-17T05:19:51.160978Z",
     "shell.execute_reply": "2025-02-17T05:19:51.160042Z",
     "shell.execute_reply.started": "2025-02-17T05:19:51.157463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\n",
    "test_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:34:24.754034Z",
     "iopub.status.busy": "2025-02-17T05:34:24.753681Z",
     "iopub.status.idle": "2025-02-17T05:34:24.976212Z",
     "shell.execute_reply": "2025-02-17T05:34:24.975375Z",
     "shell.execute_reply.started": "2025-02-17T05:34:24.754004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=file_path,split=\"train\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset from the local CSV file\n",
    "dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\", split=\"train\")\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    full_lang = map_dict.get(example[\"language\"], example[\"language\"])\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"system\", \"value\": \"You are a multilingual sentiment analysis assistant trained for 13 Indian languages. Your role is to accurately classify sentiment as Positive or Negative based on textual input, considering context, tone, and linguistic nuances specific to each language. Handle language-specific expressions, idioms, and sentiments, and ensure consistency across all languages. Provide clear and concise sentiment labels while acknowledging language diversity and ensuring fairness and accuracy in sentiment classification.\"},\n",
    "            {\"from\": \"human\", \"value\": example[\"sentence\"]},\n",
    "            {\"from\": \"gpt\", \"value\": example[\"label\"]}\n",
    "        ],\n",
    "        \"language\": full_lang  # Keeping the language field\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the transformation\n",
    "formatted_dataset = dataset.map(format_example, remove_columns=dataset.column_names)\n",
    "\n",
    "# Display the first example to verify\n",
    "print(formatted_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:52:18.562867Z",
     "iopub.status.busy": "2025-02-17T05:52:18.562512Z",
     "iopub.status.idle": "2025-02-17T05:52:18.567171Z",
     "shell.execute_reply": "2025-02-17T05:52:18.566162Z",
     "shell.execute_reply.started": "2025-02-17T05:52:18.562840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:52:28.593482Z",
     "iopub.status.busy": "2025-02-17T05:52:28.593171Z",
     "iopub.status.idle": "2025-02-17T05:52:28.960765Z",
     "shell.execute_reply": "2025-02-17T05:52:28.959847Z",
     "shell.execute_reply.started": "2025-02-17T05:52:28.593459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "formatted_dataset = standardize_sharegpt(formatted_dataset)\n",
    "formatted_dataset = formatted_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:52:36.453522Z",
     "iopub.status.busy": "2025-02-17T05:52:36.453197Z",
     "iopub.status.idle": "2025-02-17T05:52:36.459463Z",
     "shell.execute_reply": "2025-02-17T05:52:36.458635Z",
     "shell.execute_reply.started": "2025-02-17T05:52:36.453488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "formatted_dataset[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T05:36:19.494818Z",
     "iopub.status.busy": "2025-02-17T05:36:19.494388Z",
     "iopub.status.idle": "2025-02-17T05:36:26.366162Z",
     "shell.execute_reply": "2025-02-17T05:36:26.365419Z",
     "shell.execute_reply.started": "2025-02-17T05:36:19.494788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\", \n",
    "    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T06:39:06.372019Z",
     "iopub.status.busy": "2025-02-17T06:39:06.371645Z",
     "iopub.status.idle": "2025-02-17T06:39:06.607703Z",
     "shell.execute_reply": "2025-02-17T06:39:06.606911Z",
     "shell.execute_reply.started": "2025-02-17T06:39:06.371977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T06:39:06.609009Z",
     "iopub.status.busy": "2025-02-17T06:39:06.608760Z",
     "iopub.status.idle": "2025-02-17T06:39:06.616436Z",
     "shell.execute_reply": "2025-02-17T06:39:06.615600Z",
     "shell.execute_reply.started": "2025-02-17T06:39:06.608989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T06:39:08.098766Z",
     "iopub.status.busy": "2025-02-17T06:39:08.098445Z",
     "iopub.status.idle": "2025-02-17T08:01:45.010072Z",
     "shell.execute_reply": "2025-02-17T08:01:45.009053Z",
     "shell.execute_reply.started": "2025-02-17T06:39:08.098739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T08:02:08.701746Z",
     "iopub.status.busy": "2025-02-17T08:02:08.701446Z",
     "iopub.status.idle": "2025-02-17T08:02:08.719610Z",
     "shell.execute_reply": "2025-02-17T08:02:08.718602Z",
     "shell.execute_reply.started": "2025-02-17T08:02:08.701724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T08:02:15.017086Z",
     "iopub.status.busy": "2025-02-17T08:02:15.016702Z",
     "iopub.status.idle": "2025-02-17T08:02:15.026684Z",
     "shell.execute_reply": "2025-02-17T08:02:15.025688Z",
     "shell.execute_reply.started": "2025-02-17T08:02:15.017055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "sentences = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\")['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for sen in sentences:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a multilingual sentiment analysis assistant trained for 13 Indian languages. Your role is to accurately classify sentiment as Positive or Negative based on textual input.\"},\n",
    "            {\"role\": \"user\", \"content\": sen},\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True, temperature=0.1, top_p=0.9)\n",
    "        output_text = tokenizer.decode(outputs[0])\n",
    "        print(output_text)\n",
    "        # match = re.search(r'<\\\\|start_header_id\\\\|>assistant<\\\\|end_header_id\\\\|>(.*?)<\\\\|eot_id\\\\|>', output_text, re.DOTALL)\n",
    "        match = re.search(\n",
    "        r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>',\n",
    "        output_text,\n",
    "        re.DOTALL\n",
    "    )\n",
    "        if match and match.group(1):\n",
    "            labels.append(match.group(1).strip())\n",
    "        else:\n",
    "            print(\"No response found for sentence:\", sen)\n",
    "            labels.append(\"No response found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T08:04:15.864258Z",
     "iopub.status.busy": "2025-02-17T08:04:15.863965Z",
     "iopub.status.idle": "2025-02-17T08:04:15.871331Z",
     "shell.execute_reply": "2025-02-17T08:04:15.870423Z",
     "shell.execute_reply.started": "2025-02-17T08:04:15.864237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T08:04:17.068969Z",
     "iopub.status.busy": "2025-02-17T08:04:17.068640Z",
     "iopub.status.idle": "2025-02-17T08:04:17.073124Z",
     "shell.execute_reply": "2025-02-17T08:04:17.072174Z",
     "shell.execute_reply.started": "2025-02-17T08:04:17.068921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T08:04:17.416608Z",
     "iopub.status.busy": "2025-02-17T08:04:17.416283Z",
     "iopub.status.idle": "2025-02-17T08:04:17.425861Z",
     "shell.execute_reply": "2025-02-17T08:04:17.424996Z",
     "shell.execute_reply.started": "2025-02-17T08:04:17.416581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T08:04:21.973153Z",
     "iopub.status.busy": "2025-02-17T08:04:21.972797Z",
     "iopub.status.idle": "2025-02-17T08:04:21.978483Z",
     "shell.execute_reply": "2025-02-17T08:04:21.977697Z",
     "shell.execute_reply.started": "2025-02-17T08:04:21.973124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11098970,
     "sourceId": 93282,
     "sourceType": "competition"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
